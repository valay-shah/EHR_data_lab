{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import imblearn\n",
    "np.random.seed(1332)\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys                                               #### Filling the missing values using KNN because the dataset is very less\n",
    "from impyute.imputation.cs import fast_knn\n",
    "import imblearn                                      ### sampling the data using the smote method\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier              ### XGBoost classification method\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.ExcelFile('Coronna Data CERTAIN with KVB edits.xlsx')\n",
    "df = pd.read_excel(excel, 'BL+3M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['SubjectID', 'CDate', 'Match'])    ### Unique values\n",
    "df = df.drop(columns = ['SubjectID.1', 'UNMC_id.1', 'CDate.1', 'init_group.1', 'grp.1', 'UNMC_id.2', 'grp.2', 'init_group.2', 'CDate.1', 'futime.1'])    ### Duplicate values\n",
    "df = df.drop(columns = ['init_group', 'futime'])    ### same values for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['statin_use', 'rfstatus_impute', 'ccpstatus_impute', 'statin_use.1', 'smkyrs', 'numcigs'])  ### Dropping columns who have null values greater than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['3MResponse','DAS28-CRP BL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"usresultsIgA.1\": \"usresultsIgA_BL\", \"usresultsIgG.1\": \"usresultsIgG_BL\", \"usresultsIgM.1\": \"usresultsIgM_BL\", 'seatedbp1.1': 'seatedbp1_BL',\n",
    "                              'seatedbp2.1': 'seatedbp2_BL', 'pres_mtx.1': 'pres_mtx_BL', 'pres_arava.1': 'pres_arava_BL', 'pres_azulfidine.1': 'pres_azulfidine_BL', \n",
    "                              'pres_plaquenil.1': 'pres_plaquenil_BL', 'pres_imuran.1': 'pres_imuran_BL', 'pres_minocin.1': 'pres_minocin_BL', 'pres_pred.1': 'pres_pred_BL',\n",
    "                              'statin_use.1': 'statin_use_BL', 'tender_jts_28.1': 'tender_jts_28_BL', 'BLswollen_jts_28': 'swollen_jts_28_BL',\n",
    "                              'BLmd_global_assess': 'md_global_assess_BL', 'BLpt_global_assess': 'pt_global_assess_BL', 'BLdi': 'di_BL', 'BLpt_pain': 'pt_pain_BL', 'BLusresultsCRP': 'usresultsCRP_BL',\n",
    "                              'DAS28-CRP BL': 'DAS28-CRP_BL', })     ### Renaming some columns. Not sure why - did it because previous student did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-f80c6ef2a649>:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop('UNMC_id',1)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('UNMC_id',1)\n",
    "final_df = df\n",
    "model_label = LabelEncoder()\n",
    "final_df['grp'] = model_label.fit_transform(final_df['grp'].astype('str'))\n",
    "final_df['gender'] = model_label.fit_transform(final_df['gender'].astype('str'))\n",
    "final_df['final_education'] = model_label.fit_transform(final_df['final_education'].astype('str'))\n",
    "final_df['race_grp'] = model_label.fit_transform(final_df['race_grp'].astype('str'))\n",
    "final_df['newsmoker'] = model_label.fit_transform(final_df['newsmoker'].astype('str'))\n",
    "final_df['drinker'] = model_label.fit_transform(final_df['drinker'].astype('str'))\n",
    "final_df['ara_func_class'] = model_label.fit_transform(final_df['ara_func_class'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df = final_df[final_df['DAS28-CRP 3m'] != 'Unknown']    ### We don't need unknown category in our classification data so removing that category\n",
    "final_df = final_df[final_df['DAS28-CRP 3m'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-8d086750a9c9>:2: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X = final_df.drop('DAS28-CRP 3m',1)                            ### Dividing the dataframe into X and y set\n"
     ]
    }
   ],
   "source": [
    "rs = 1337\n",
    "X = final_df.drop('DAS28-CRP 3m',1)                            ### Dividing the dataframe into X and y set\n",
    "y = final_df['DAS28-CRP 3m']\n",
    "y = model_label.fit_transform(y.astype('str'))\n",
    "X_train_norm, X_val_norm, Y_train, Y_val = train_test_split(X, y, test_size=0.1,random_state = rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_norm['y'] = Y_train\n",
    "#X_val_norm['y'] = Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=15)\n",
    "fit_train = imputer.fit(X_train_norm)\n",
    "imput_train = fit_train.transform(X_train_norm)\n",
    "c = list(X_train_norm.columns)\n",
    "imput_train_df = pd.DataFrame(imput_train, columns = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_test = fit_train.transform(X_val_norm)\n",
    "c = list(X_val_norm.columns)\n",
    "imput_test_df = pd.DataFrame(imput_test, columns = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['grp','gender','final_education','race_grp','newsmoker','drinker','ara_func_class']     ### normalizing the dataset\n",
    "\n",
    "for i in imput_train_df.columns:\n",
    "    if i not in labels:\n",
    "        mean = imput_train_df[i].mean()\n",
    "        std = imput_train_df[i].std()\n",
    "        imput_train_df[i] = (imput_train_df[i] - mean) / std\n",
    "        imput_test_df[i] = (imput_test_df[i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-649ae34e813e>:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  imput_train_df = imput_train_df.drop(i,1)\n",
      "<ipython-input-26-649ae34e813e>:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  imput_test_df = imput_test_df.drop(i,1)\n"
     ]
    }
   ],
   "source": [
    "labels = ['pres_imuran','pres_minocin','num_tnf','num_nontnf','hxunstab_ang','pres_minocin_BL','ethnicity','hxstroke','pres_imuran_BL']    #### This columns has same value for every row so when I divide by mean and std the value goes to infinity\n",
    "for i in labels:\n",
    "    imput_train_df = imput_train_df.drop(i,1)\n",
    "    imput_test_df = imput_test_df.drop(i,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sampling(over_under,X,y):\n",
    "#     if over_under == 'under':\n",
    "#         sampler = imblearn.under_sampling.RandomUnderSampler()\n",
    "#         X_under, y_under = sampler.fit_resample(X, y)\n",
    "#         return X_under,y_under\n",
    "#     elif over_under == 'over':\n",
    "#         ros = RandomOverSampler(random_state=rs)\n",
    "#         X_over, y_over = ros.fit_resample(X, y)\n",
    "#         return X_over,y_over\n",
    "#     else:\n",
    "#         sampler = SMOTE(random_state = rs)\n",
    "#         X_smote, y_smote = sampler.fit_resample(X, y)\n",
    "#         return X_smote,y_smote\n",
    "# X_final,y_final = sampling('smote',imput_train_df,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_train_df['y'] = Y_train\n",
    "imput_test_df['y'] = Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220405_161759\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220405_161759\\\"\n",
      "AutoGluon Version:  0.4.0\n",
      "Python Version:     3.8.5\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    250\n",
      "Train Data Columns: 68\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (277, 0, 142.336, 80.44499)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    7818.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.14 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 21 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 68 | ['grp', 'Type I IFN activity', 'IFNβ activity final', 'IFNα activity final', 'IFNβ/α ratio final', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 47 | ['grp', 'Type I IFN activity', 'IFNβ activity final', 'IFNα activity final', 'IFNβ/α ratio final', ...]\n",
      "\t\t('int', ['bool']) : 21 | ['pres_mtx', 'pres_arava', 'pres_azulfidine', 'pres_plaquenil', 'pres_pred', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t68 features in original data used to generate 68 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.1 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 200, Val Rows: 50\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.3375\t = Validation score   (r2)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.3375\t = Validation score   (r2)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.9148\t = Validation score   (r2)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.9161\t = Validation score   (r2)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ...\n",
      "\t0.8041\t = Validation score   (r2)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.9262\t = Validation score   (r2)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ...\n",
      "\t0.8719\t = Validation score   (r2)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8407\t = Validation score   (r2)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.8665\t = Validation score   (r2)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7414\t = Validation score   (r2)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.884\t = Validation score   (r2)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.9394\t = Validation score   (r2)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 8.77s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220405_161759\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "predictor = TabularPredictor(label='y', eval_metric = 'r2').fit(imput_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: r2 on test data: 0.9637536615980647\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"r2\": 0.9637536615980647,\n",
      "    \"root_mean_squared_error\": -13.581291072465865,\n",
      "    \"mean_squared_error\": -184.451467195041,\n",
      "    \"mean_absolute_error\": -11.839905670710973,\n",
      "    \"pearsonr\": 0.9867651792010529,\n",
      "    \"median_absolute_error\": -11.967399597167969\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'r2': 0.9637536615980647,\n",
       " 'root_mean_squared_error': -13.581291072465865,\n",
       " 'mean_squared_error': -184.451467195041,\n",
       " 'mean_absolute_error': -11.839905670710973,\n",
       " 'pearsonr': 0.9867651792010529,\n",
       " 'median_absolute_error': -11.967399597167969}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(imput_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"model-weights\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"model-weights\\\"\n",
      "AutoGluon Version:  0.4.0\n",
      "Python Version:     3.8.5\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    279\n",
      "Train Data Columns: 68\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t3 unique label values:  [2, 1, 0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5202.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 68 | ['grp', 'Type I IFN activity', 'IFNβ activity final', 'IFNα activity final', 'IFNβ/α ratio final', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 64 | ['grp', 'Type I IFN activity', 'IFNβ activity final', 'IFNα activity final', 'IFNβ/α ratio final', ...]\n",
      "\t\t('int', ['bool']) :  4 | ['pres_arava', 'hxchf', 'hxtia', 'hxcopd']\n",
      "\t0.1s = Fit runtime\n",
      "\t68 features in original data used to generate 68 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.14 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 223, Val Rows: 56\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.5\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.5536\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 2: early stopping\n",
      "\t0.7143\t = Validation score   (accuracy)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.75\t = Validation score   (accuracy)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.75\t = Validation score   (accuracy)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6607\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6607\t = Validation score   (accuracy)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.6964\t = Validation score   (accuracy)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.625\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.6607\t = Validation score   (accuracy)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.75\t = Validation score   (accuracy)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.625\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.75\t = Validation score   (accuracy)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.75\t = Validation score   (accuracy)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 8.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"model-weights\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "save_path = 'model-weights'\n",
    "predictor = TabularPredictor(label='y', path=save_path).fit(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "#y_test = test_data[label]  # values to predict\n",
    "#test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val = pd.Series(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.25\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.25,\n",
      "    \"balanced_accuracy\": 0.3636363636363637,\n",
      "    \"mcc\": -0.029821603968282907\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  \n",
      " 8      1\n",
      "195    1\n",
      "174    2\n",
      "72     2\n",
      "242    2\n",
      "118    2\n",
      "206    2\n",
      "196    1\n",
      "288    2\n",
      "295    2\n",
      "198    1\n",
      "106    2\n",
      "129    2\n",
      "213    1\n",
      "3      2\n",
      "175    2\n",
      "13     1\n",
      "69     2\n",
      "202    2\n",
      "18     2\n",
      "163    2\n",
      "23     1\n",
      "265    2\n",
      "287    1\n",
      "87     2\n",
      "56     2\n",
      "149    2\n",
      "78     2\n",
      "Name: y, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "\n",
    "y_pred = predictor.predict(X_val_norm)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=Y_val, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\n",
    "subsample_size = 500  # subsample subset of data for faster demo, try setting this to much larger values\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'class'\n",
    "print(\"Summary of class variable: \\n\", train_data[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
    "predictor = TabularPredictor(label=label, path=save_path).fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\n",
    "y_test = test_data[label]  # values to predict\n",
    "print(type(y_test), y_test)\n",
    "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
    "test_data_nolab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained predictor from file\n",
    "\n",
    "y_pred = predictor.predict(test_data_nolab)\n",
    "print(\"Predictions:  \\n\", y_pred)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220405_154934\\\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220405_154934\\\"\n",
      "AutoGluon Version:  0.4.0\n",
      "Python Version:     3.8.5\n",
      "Operating System:   Windows\n",
      "Train Data Rows:    250\n",
      "Train Data Columns: 77\n",
      "Label Column: y\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t3 unique label values:  [2, 1, 0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8610.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.15 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 23 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tUseless Original Features (Count: 6): ['pres_imuran', 'pres_minocin', 'num_tnf', 'num_nontnf', 'hxunstab_ang', 'pres_minocin_BL']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 40 | ['Type I IFN activity', 'IFNβ activity final', 'IFNα activity final', 'IFNβ/α ratio final', 'seatedbp1', ...]\n",
      "\t\t('int', [])   : 31 | ['grp', 'age', 'gender', 'final_education', 'race_grp', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 35 | ['Type I IFN activity', 'IFNβ activity final', 'IFNα activity final', 'IFNβ/α ratio final', 'seatedbp1', ...]\n",
      "\t\t('int', [])       : 13 | ['grp', 'age', 'final_education', 'race_grp', 'height', ...]\n",
      "\t\t('int', ['bool']) : 23 | ['pres_mtx', 'pres_arava', 'pres_azulfidine', 'pres_plaquenil', 'pres_pred', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t71 features in original data used to generate 71 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.1 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 200, Val Rows: 50\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif ...\n",
      "\t0.4\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ...\n",
      "\t0.38\t = Validation score   (accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 3: early stopping\n",
      "\t0.64\t = Validation score   (accuracy)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ...\n",
      "\t0.68\t = Validation score   (accuracy)\n",
      "\t1.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ...\n",
      "\t0.6\t = Validation score   (accuracy)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.52\t = Validation score   (accuracy)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.56\t = Validation score   (accuracy)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost ...\n",
      "\t0.64\t = Validation score   (accuracy)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ...\n",
      "\t0.5\t = Validation score   (accuracy)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ...\n",
      "\t0.54\t = Validation score   (accuracy)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: XGBoost ...\n",
      "\t0.6\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.68\t = Validation score   (accuracy)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ...\n",
      "\t0.6\t = Validation score   (accuracy)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.7\t = Validation score   (accuracy)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 9.77s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220405_154934\\\")\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "#excel = pd.ExcelFile('Coronna Data CERTAIN with KVB edits.xlsx')\n",
    "#train_data = pd.read_excel(excel, 'BL+3M')\n",
    "predictor = TabularPredictor(label='y').fit(X_train_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.8571428571428571\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.8571428571428571,\n",
      "    \"balanced_accuracy\": 0.8535353535353535,\n",
      "    \"mcc\": 0.7848854747681295\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8571428571428571,\n",
       " 'balanced_accuracy': 0.8535353535353535,\n",
       " 'mcc': 0.7848854747681295}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(X_val_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
