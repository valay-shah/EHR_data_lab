{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100,200,300,400,500,600],\n",
    "    'max_depth': [5,10,15,30,40,80, 100],\n",
    "    'eta':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    'min_child_weight':[0,1,2,3,4,5,6,7,8,9],\n",
    "    'sampling_method': ['uniform', 'gradient_based'],\n",
    "    'colsample_bytree': [0.5,0.6,0.7,0.8],\n",
    "    'reg_alpha': [30,35,40,45,50,55,60,65,70,75,80],\n",
    "    'reg_lambda': [0.1,0.2,0.3,0.4,0.5,0.6,0.7]\n",
    "    'min_samples_leaf': [3,6,10,15,20,30, 40],\n",
    "    'min_samples_split': [3,6,10, 15, 20],\n",
    "    'criterion': ['entropy','gini'],\n",
    "    'max_features': ['auto','sqrt','log2']\n",
    "    \n",
    "}\n",
    "params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "#params\n",
    "rand = sklearn.ensemble.RandomForestClassifier(**params,n_jobs=-1)\n",
    "acc = cross_val_score(rand,X_final , y_final,scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "r_best = acc\n",
    "random_results = pd.DataFrame(columns = ['loss', 'params', 'iteration', 'estimators', 'time'],\n",
    "                       index = list(range(1000)))\n",
    "subsample_dist = list(np.linspace(0.5, 1, 100))\n",
    "\n",
    "def random_objective(params, iteration, n_folds = 10):\n",
    "    \"\"\"Random search objective function. Takes in hyperparameters\n",
    "       and returns a list of results to be saved.\"\"\"\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = cross_val_score(rand,X_final , y_final,scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "    end = timer()\n",
    "    best_score = np.max(cv_results)\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results) + 1)\n",
    "    \n",
    "    # Return list of results\n",
    "    return [loss, params, iteration, n_estimators, end - start]\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Randomly sample parameters for gbm\n",
    "    params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "    \n",
    "    #print(params)\n",
    "    params['subsample'] = random.sample(subsample_dist, 1)[0]\n",
    "        \n",
    "        \n",
    "    results_list = random_objective(params, i)\n",
    "    \n",
    "    # Add results to next row in dataframe\n",
    "    random_results.loc[i, :] = results_list\n",
    "    \n",
    "    \n",
    "random_results.sort_values('loss', ascending = True, inplace = True)\n",
    "random_results.reset_index(inplace = True, drop = True)\n",
    "random_results.head()\n",
    "random_results['params'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "XGB = XGBClassifier(random_state=0)\n",
    "parameter_grid = {\n",
    "    'n_estimators': [100,200,300,400,500,600],\n",
    "    'max_depth': [5,10,15,30,40,80],\n",
    "    'min_samples_leaf': [3,6,10,15,20,30],\n",
    "    'min_samples_split': [3,6,10, 15, 20],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_features': ['auto','sqrt','log2']\n",
    "    \n",
    "}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10,random_state=0,shuffle=True)\n",
    "\n",
    "grid_search_XGB = GridSearchCV(XGB,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation,\n",
    "                            n_jobs=-1,\n",
    "                            verbose=0)\n",
    "\n",
    "grid_search_XGB.fit(X_final, y_final)\n",
    "print('Best score: {}'.format(grid_search_XGB.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search_XGB.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-optimize\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "space = [\n",
    "Integer(100,1000,name = \"n_estimators\"),\n",
    "Integer( 10, 80, name = \"max_depth\"),\n",
    "Categorical([\"auto\",\"sqrt\",\"log2\"], name = \"max_features\"),\n",
    "Categorical([\"gini\",\"entropy\"], name = \"criterion\"),\n",
    "Integer(4, 15, name = \"min_samples_split\"),\n",
    "Integer(5, 9, name = \"min_samples_leaf\")\n",
    "#Categorical([True,False], name = \"bootstrap\")\n",
    "    \n",
    "]\n",
    "tune_rand_gp= None\n",
    "print(tune_rand_gp)\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    rand = sklearn.ensemble.RandomForestClassifier(**params,n_jobs=-1)\n",
    "    return -np.mean(cross_val_score(rand, X_final , y_final, cv=5,   \n",
    "                    n_jobs=-1,\n",
    "                    scoring=\"neg_mean_absolute_error\"))\n",
    "tune_rand_gp = gp_minimize(objective,space,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score\n",
    "space = {\n",
    "\"n_estimators\": hp.choice(\"n_estimators\",[200,600,900,1200,1500]),\n",
    "\"max_depth\": hp.quniform(\"max_depth\", 10, 80,5),\n",
    "\"max_features\": hp.choice(\"max_features\", [\"auto\",\"sqrt\",\"log2\"]),\n",
    "\"criterion\": hp.choice(\"criterion\", [\"gini\",\"entropy\"]),\n",
    "\"min_samples_split\":hp.choice(\"min_samples_split\",[2, 5, 10,12,15]),\n",
    "\"min_samples_leaf\":hp.choice(\"min_samples_leaf\",[1, 2, 4,7,9])}\n",
    "def tune_random(params):\n",
    "    rand = sklearn.ensemble.RandomForestClassifier(**params,n_jobs=-1)\n",
    "    acc = cross_val_score(rand,X_final , y_final,scoring=\"accuracy\", n_jobs=-1).mean()\n",
    "    return {\"loss\": -acc, \"status\": STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(fn=tune_random, space = space, algo=tpe.suggest,\n",
    "            max_evals=100, trials=trials)\n",
    "print(\"Best: {}\".format(best))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
