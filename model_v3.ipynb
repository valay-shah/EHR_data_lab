{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9e1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import imblearn\n",
    "np.random.seed(1332)\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys                                               #### Filling the missing values using KNN because the dataset is very less\n",
    "from impyute.imputation.cs import fast_knn\n",
    "import imblearn                                      ### sampling the data using the smote method\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier              ### XGBoost classification method\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677e9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.ExcelFile('Coronna Data CERTAIN with KVB edits.xlsx')\n",
    "df = pd.read_excel(excel, 'BL+3M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d09ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f71cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['SubjectID', 'CDate', 'Match'])    ### Unique values\n",
    "df = df.drop(columns = ['SubjectID.1', 'UNMC_id.1', 'CDate.1', 'init_group.1', 'grp.1', 'UNMC_id.2', 'grp.2', 'init_group.2', 'CDate.1', 'futime.1'])    ### Duplicate values\n",
    "df = df.drop(columns = ['init_group', 'futime'])    ### same values for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4412378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().mean()[df.isnull().mean() > 0.7]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9098380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['statin_use', 'rfstatus_impute', 'ccpstatus_impute', 'statin_use.1', 'smkyrs', 'numcigs'])  ### Dropping columns who have null values greater than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466ead4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['DAS28-CRP 3m', 'DAS28-CRP BL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f2e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"usresultsIgA.1\": \"usresultsIgA_BL\", \"usresultsIgG.1\": \"usresultsIgG_BL\", \"usresultsIgM.1\": \"usresultsIgM_BL\", 'seatedbp1.1': 'seatedbp1_BL',\n",
    "                              'seatedbp2.1': 'seatedbp2_BL', 'pres_mtx.1': 'pres_mtx_BL', 'pres_arava.1': 'pres_arava_BL', 'pres_azulfidine.1': 'pres_azulfidine_BL', \n",
    "                              'pres_plaquenil.1': 'pres_plaquenil_BL', 'pres_imuran.1': 'pres_imuran_BL', 'pres_minocin.1': 'pres_minocin_BL', 'pres_pred.1': 'pres_pred_BL',\n",
    "                              'statin_use.1': 'statin_use_BL', 'tender_jts_28.1': 'tender_jts_28_BL', 'BLswollen_jts_28': 'swollen_jts_28_BL',\n",
    "                              'BLmd_global_assess': 'md_global_assess_BL', 'BLpt_global_assess': 'pt_global_assess_BL', 'BLdi': 'di_BL', 'BLpt_pain': 'pt_pain_BL', 'BLusresultsCRP': 'usresultsCRP_BL',\n",
    "                              'DAS28-CRP BL': 'DAS28-CRP_BL', })     ### Renaming some columns. Not sure why - did it because previous student did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c68bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-14953214/ipykernel_11681/2061900728.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop('UNMC_id',1)\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('UNMC_id',1)\n",
    "final_df = df\n",
    "model_label = LabelEncoder()\n",
    "final_df['grp'] = model_label.fit_transform(final_df['grp'].astype('str'))\n",
    "final_df['gender'] = model_label.fit_transform(final_df['gender'].astype('str'))\n",
    "final_df['final_education'] = model_label.fit_transform(final_df['final_education'].astype('str'))\n",
    "final_df['race_grp'] = model_label.fit_transform(final_df['race_grp'].astype('str'))\n",
    "final_df['newsmoker'] = model_label.fit_transform(final_df['newsmoker'].astype('str'))\n",
    "final_df['drinker'] = model_label.fit_transform(final_df['drinker'].astype('str'))\n",
    "final_df['ara_func_class'] = model_label.fit_transform(final_df['ara_func_class'].astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01746f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['3MResponse'] != 'Unknown']### We don't need unknown category in our classification data so removing that category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9160fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "# def z_score(data):\n",
    "#     data_new = data.drop('3MResponse',1)\n",
    "#     z_scores = stats.zscore(data_new)\n",
    "#     abs_z_scores = np.abs(z_scores)\n",
    "#     print(data_new)\n",
    "#     filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "#     print(filtered_entries.value_counts())\n",
    "#     new_df = data_new[filtered_entries]\n",
    "#     print(new_df.shape)\n",
    "#     return new_df\n",
    "# X = z_score(final_df)\n",
    "# y = final_df.iloc[X.index,-1]\n",
    "# y = model_label.fit_transform(y.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf4ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-14953214/ipykernel_11681/42841613.py:1: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  X = final_df.drop('3MResponse',1)                            ### Dividing the dataframe into X and y set\n"
     ]
    }
   ],
   "source": [
    "X = final_df.drop('3MResponse',1)                            ### Dividing the dataframe into X and y set\n",
    "y = final_df['3MResponse']\n",
    "#y = model_label.fit_transform(y.astype('str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a69c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(new_df):\n",
    "    sys.setrecursionlimit(100000) \n",
    "    imputed_training=fast_knn(new_df.values, k=15)\n",
    "    return imputed_training\n",
    "imputed_training = KNN(X)\n",
    "c = list(X.columns)\n",
    "df_without_nulls = pd.DataFrame(imputed_training, columns=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb57603",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = ['grp','gender','final_education','race_grp','newsmoker','drinker','ara_func_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42068a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 77)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "for i in df_without_nulls.columns:\n",
    "    if i not in lab:\n",
    "        IQR = df_without_nulls[i].quantile(0.75) - df_without_nulls[i].quantile(0.25)\n",
    "        lower_limit = df_without_nulls[i].quantile(0.25) - 1.5 * IQR\n",
    "        upr_limit = df_without_nulls[i].quantile(0.75) + 1.5 * IQR\n",
    "        upr = df_without_nulls[i] > upr_limit\n",
    "        low = df_without_nulls[i] < lower_limit\n",
    "        #df_without_nulls = df_without_nulls[(df_without_nulls[i] < upr_limit) and (df_without_nulls[i] > lower_limit)].all()\n",
    "        df_without_nulls = df_without_nulls[~low | upr]\n",
    "        \n",
    "print(df_without_nulls.shape)     \n",
    "        \n",
    "# def z_score(data_new):\n",
    "#     z_scores = stats.zscore(data_new)\n",
    "#     abs_z_scores = np.abs(z_scores)\n",
    "#     print(data_new)\n",
    "#     filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "#     print(filtered_entries.value_counts())\n",
    "#     new_df = data_new[filtered_entries]\n",
    "#     print(new_df.shape)\n",
    "#     return new_df\n",
    "# X_1 = z_score(df_without_nulls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0894e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df_without_nulls.columns:\n",
    "#     if df_without_nulls[i].isnull().sum() == 155:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111edf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655cee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_nulls.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = final_df.iloc[df_without_nulls.index,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e578226",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bce37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = \n",
    "y= final_df.iloc[df_without_nulls.index,0]\n",
    "y = model_label.fit_transform(y.astype('str'))\n",
    "\n",
    "#print(y.shape, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a6bb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 1, 0, 2, 1, 1, 2, 0, 2, 1, 0, 1, 2,\n",
       "       1, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 2, 2, 1, 0, 1, 1, 0, 1, 2, 0, 1, 1, 0, 2, 2, 2, 2, 0,\n",
       "       1, 1, 1, 0, 0, 1, 2, 2, 2, 2, 1, 0, 2, 1, 0, 2, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 1,\n",
       "       2, 1, 1, 0, 1, 2, 0, 0, 0, 2, 1, 0, 2, 2, 1, 2, 2, 0, 1, 2, 0, 1,\n",
       "       2, 1, 0, 0, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1, 0, 0, 1, 0, 2, 1, 1, 2,\n",
       "       0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94116c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['grp','gender','final_education','race_grp','newsmoker','drinker','ara_func_class']     ### normalizing the dataset\n",
    "\n",
    "for i in df_without_nulls.columns:\n",
    "    if i not in labels:\n",
    "        mean = df_without_nulls[i].mean()\n",
    "        std = df_without_nulls[i].std()\n",
    "        df_without_nulls[i] = (df_without_nulls[i] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4c8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-14953214/ipykernel_11681/1131187374.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df_without_nulls = df_without_nulls.drop(i,1)\n"
     ]
    }
   ],
   "source": [
    "labels = ['pres_imuran','pres_minocin','num_tnf','num_nontnf','hxunstab_ang','pres_minocin_BL','ethnicity','hxstroke','pres_imuran_BL']    #### This columns has same value for every row so when I divide by mean and std the value goes to infinity\n",
    "for i in labels:\n",
    "    df_without_nulls = df_without_nulls.drop(i,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c33a4e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29393409",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm, X_val_norm, Y_train, Y_val = train_test_split(df_without_nulls, y, random_state=rs, test_size=0.1)     ### splitting the dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11218070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(over_under,X,y):\n",
    "    if over_under == 'under':\n",
    "        sampler = imblearn.under_sampling.RandomUnderSampler(random_state = rs)\n",
    "        X_under, y_under = sampler.fit_resample(X, y)\n",
    "        return X_under,y_under\n",
    "    elif over_under == 'over':\n",
    "        ros = RandomOverSampler(random_state=rs)\n",
    "        X_over, y_over = ros.fit_resample(X, y)\n",
    "        return X_over,y_over\n",
    "    else:\n",
    "        sampler = SMOTE()\n",
    "        X_smote, y_smote = sampler.fit_resample(X, y)\n",
    "        return X_smote,y_smote\n",
    "#print(X)\n",
    "#print(y)\n",
    "X_final,y_final = sampling('smote',X_train_norm,Y_train)\n",
    "#X_final,y_final = X_train_norm, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18f11e3",
   "metadata": {},
   "source": [
    "arr = [0.8214, 0.5714, 0.7142, 0.5714, 0.75, 0.6428, 0.8214, 0.7857, 0.5357,0.75]\n",
    "arr1 = [0.7195, 0.7378, 0.7440, 0.7369, 0.7104, 0.7250, 0.7163, 0.7063, 0.7623, 0.7487]\n",
    "print(np.std(arr), np.std(arr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "113fae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6839080459770115\n",
      "Training accuracy is 0.9655172413793104\n",
      "Testing accuracy is 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/base.py:445: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model3 = None\n",
    "#print(model3)\n",
    "def random_forest(X_train_norm, X_test, Y_train,Y_test):\n",
    "    global model3\n",
    "    #print(model3)\n",
    "    model3 = sklearn.ensemble.RandomForestClassifier(n_estimators=474, oob_score = True, criterion='gini',max_depth=90, min_samples_split=10, random_state=rs, max_features='auto', min_samples_leaf = 6)\n",
    "    model3.fit(X_train_norm, Y_train)\n",
    "    print(model3.oob_score_)\n",
    "    print(f\"Training accuracy is {model3.score(X_train_norm, Y_train)}\")\n",
    "    print(f\"Testing accuracy is {model3.score(X_test, Y_test)}\")\n",
    "    #sklearn.metrics.plot_confusion_matrix(model3, X_test, Y_test)\n",
    "    #sklearn.metrics.plot_roc_curve(model3, X_test, Y_test)\n",
    "random_forest(X_final, X_val_norm, y_final, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17b7fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn::: 0.7415032679738561\n",
      "Training accuracy is 0.9367816091954023\n",
      "Testing accuracy is 0.3125\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "model_svm = None\n",
    "def svm1(X_train_norm, X_test, Y_train,Y_test):\n",
    "    global model_svm\n",
    "    model_svm = svm.SVC(random_state = rs, kernel = 'poly')\n",
    "    model_svm.fit(X_train_norm, Y_train)\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    results = cross_val_score(model_svm, X_train_norm, Y_train, cv=10)\n",
    "    print(\"sklearn:::\", results.mean())\n",
    "    print(f\"Training accuracy is {model_svm.score(X_train_norm, Y_train)}\")\n",
    "    print(f\"Testing accuracy is {model_svm.score(X_test, Y_test)}\")\n",
    "    \n",
    "svm1(X_final, X_val_norm, y_final, Y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9324deba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/ext3/miniconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:56:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "sklearn::: 0.7758169934640523\n",
      "Training accuracy is 1.0\n",
      "Testing accuracy is 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "lr = sklearn.linear_model.LogisticRegression()  # defining meta-classifier\n",
    "KNC = XGBClassifier()   # initialising KNeighbors Classifier\n",
    "NB = sklearn.ensemble.RandomForestClassifier()\n",
    "clf_stack = StackingClassifier(classifiers =[KNC, NB], meta_classifier = lr, use_probas = True, use_features_in_secondary = True)\n",
    "clf_stack = clf_stack.fit(X_final, y_final)\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "results = cross_val_score(clf_stack, X_final, y_final, cv=10)\n",
    "print(\"sklearn:::\", results.mean())\n",
    "print(f\"Training accuracy is {clf_stack.score(X_final, y_final)}\")\n",
    "print(f\"Testing accuracy is {clf_stack.score(X_val_norm, Y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model5 = None\n",
    "def logistic_regress(X_train_norm, X_test, Y_train,Y_test):\n",
    "    global model5\n",
    "    model5 = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='saga', penalty = 'elasticnet', C = 1, l1_ratio = 0.5, random_state=rs)\n",
    "    model5.fit(X_train_norm, Y_train)\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    results = cross_val_score(model5, X_train_norm, Y_train, cv=10)\n",
    "    print(\"sklearn:::\", results.mean())\n",
    "    print(f\"Training accuracy is {model5.score(X_train_norm, Y_train)}\")\n",
    "    print(f\"Testing accuracy is {model5.score(X_test, Y_test)}\")\n",
    "    #sklearn.metrics.plot_confusion_matrix(model3, X_test, Y_test)\n",
    "    #sklearn.metrics.plot_roc_curve(model3, X_test, Y_test)\n",
    "logistic_regress(X_final, X_val_norm, y_final, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dmatrix = xgboost.DMatrix(data=X_final,label=y_final)\n",
    "model4 = None\n",
    "def xgboost(X_train_norm, X_test,Y_train, Y_test):\n",
    "    global model4\n",
    "    #model4 = XGBClassifier(n_estimators = 300, random_state=rs, max_depth=100, eta=0.5, gamma=15, min_child_weight=4, alpha=44, reg_lambda=0.44, colsample_bytree = 0.5, sampling_method = 'uniform')\n",
    "    model4 = XGBClassifier(n_estimators = 200, random_state=rs, subsample=0.8, max_depth=50, eta=0.5, gamma=1, min_child_weight=10, alpha=1)\n",
    "\n",
    "    model4.fit(X_train_norm, Y_train)\n",
    "    kfold = StratifiedKFold(n_splits=10)\n",
    "    results = cross_val_score(model4, X_train_norm, Y_train, cv=10)\n",
    "    print(\"sklearn:::\", results, results.mean())\n",
    "    print(f\"Training accuracy is {model4.score(X_train_norm, Y_train)}\")\n",
    "    print(f\"Testing accuracy is {model4.score(X_test, Y_test)}\")\n",
    "xgboost(X_final, X_val_norm, y_final, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (0,1,2)\n",
    "y = (70.99, 67.85, 70.01)\n",
    "plt.figure()\n",
    "yerr = np.array([(2.81,1.5),(3.6, 3.7), (2.6, 3.1)]).T\n",
    "plt.errorbar(x, y, yerr, fmt='r^')\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.title(\"validation/oob score\")\n",
    "plt.text(3.5, 70, '0 = Logistric Regression', fontsize = 23)\n",
    "plt.text(3.5, 69, '1 = Random Forest', fontsize = 23)\n",
    "plt.text(3.5, 68, '2 = XGBoost', fontsize = 23)\n",
    "plt.xticks([0,1,2])\n",
    "\n",
    "#plt.legend(['First line', 'Second line','Third Line'])\n",
    "plt.show()\n",
    "\n",
    "x_test = [0, 1, 2]\n",
    "y_test = [66.42, 64.63, 64.63]\n",
    "plt.figure()\n",
    "y_err_test = np.array([(16.42, 12.14),(21.78, 13.93), (21.78, 13.93)]).T\n",
    "plt.errorbar(x_test, y_test, y_err_test, fmt='r^')\n",
    "plt.title(\"test score\")\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.text(3.5, 70, '0 = Logistric Regression', fontsize = 23)\n",
    "plt.text(3.5, 65, '1 = Random Forest', fontsize = 23)\n",
    "plt.text(3.5, 60, '2 = XGBoost', fontsize = 23)\n",
    "plt.xticks([0,1,2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd185f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
